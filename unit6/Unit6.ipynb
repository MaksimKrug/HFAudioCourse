{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fbaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login(new_session=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebdd5c4",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a720b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = load_dataset(\"facebook/voxpopuli\", \"pl\", split=\"train\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5791f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessor\n",
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81778e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match dataset vocab\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"normalized_text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "\n",
    "vocabs = dataset.map(\n",
    "    extract_all_chars,\n",
    "    batched=True,\n",
    "    batch_size=-1,\n",
    "    keep_in_memory=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
    "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}\n",
    "print(dataset_vocab - tokenizer_vocab)\n",
    "\n",
    "replacements = [(\"1\", \"l\"), (\"ó\", \"u\"), (\"ą\", \"on\"), (\"ć\", \"ch\"),\n",
    "                (\"ę\", \"en\"), (\"ł\", \"w\"), (\"ń\", \"ny\"), (\"ś\", \"sh\"),\n",
    "                (\"ź\", \"zh\"), (\"ż\", \"zh\")]\n",
    "\n",
    "\n",
    "def cleanup_text(inputs):\n",
    "    for src, dst in replacements:\n",
    "        inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "dataset = dataset.map(cleanup_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93021d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by speakers number\n",
    "speaker_counts = defaultdict(int)\n",
    "\n",
    "for speaker_id in dataset[\"speaker_id\"]:\n",
    "    speaker_counts[speaker_id] += 1\n",
    "    \n",
    "def select_speaker(speaker_id):\n",
    "    return 100 <= speaker_counts[speaker_id] <= 400\n",
    "\n",
    "dataset = dataset.filter(select_speaker, input_columns=[\"speaker_id\"])\n",
    "len(set(dataset[\"speaker_id\"])), len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings for speakers\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
    ")\n",
    "\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d509ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Processing dataset\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        text=example[\"normalized_text\"],\n",
    "        audio_target=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # strip off the batch dimension\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "\n",
    "    # use SpeechBrain to obtain x-vector\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove too long examples\n",
    "def is_not_too_long(input_ids):\n",
    "    input_length = len(input_ids)\n",
    "    return input_length < 200\n",
    "\n",
    "\n",
    "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a45d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d60d03",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cffcde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "\n",
    "        # collate the inputs and targets into a batch\n",
    "        batch = processor.pad(\n",
    "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
    "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
    "        )\n",
    "\n",
    "        # not used during fine-tuning\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        # round down target lengths to multiple of reduction factor\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor(\n",
    "                [len(feature[\"input_values\"]) for feature in label_features]\n",
    "            )\n",
    "            target_lengths = target_lengths.new(\n",
    "                [\n",
    "                    length - length % model.config.reduction_factor\n",
    "                    for length in target_lengths\n",
    "                ]\n",
    "            )\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "\n",
    "        # also add in the speaker embeddings\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "\n",
    "        return batch\n",
    "    \n",
    "data_collator = TTSDataCollatorWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7dca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(model.generate, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"speecht5_finetuned_voxpopuli_pl\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=6000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "#     report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b897bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32194d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to hub\n",
    "kwargs = {\n",
    "    \"dataset_tags\": \"facebook/voxpopuli\",\n",
    "    \"dataset\": \"VOXPOPULI\",\n",
    "    \"model_name\": f\"text-to-speech-finetuned-voxpopuli-pl\",\n",
    "    \"finetuned_from\": checkpoint,\n",
    "    \"tasks\": \"text-to-speech\",\n",
    "    \"tags\": \"text-to-speech\"\n",
    "}\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea3331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776003e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab11758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
